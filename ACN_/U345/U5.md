Introduction to Software Defined Networks The network devices like Router,Swith are comprised of Data plane:All the data packet moving activities like packet forwaring, Segmentation & reassembly of data, Replication of packets for multicasting belongs to Data plane. Control Plane: All the activities that controls the data plane activities belongs to this plane. Control plane do not involve end user data packets. It is the brain of the network. Construction of routing table, building packet handling policies are some of control plane activities. Network device come with interfaces like CLI, GUI, Netconf etc., that allow a network operator to configure and manage these devices. Interfaces provide options that allows an operator to a access the device’s capabilities, but they still often hide the lowest levels of details from the operator. To program these network devices the operator has to use the syntax or semantics of functionality that exists in a device. Introduction to SDN Distributed Control Plane: In conventional networks, every switch has its own data plane and control plane. The control plane of various switches exchange topology information and constructs a routing table to decides where an incoming data packet has to be forwarded via the data plane. As routing table is constructed individually this is called Distributed Control Plane Centralized Control Plane: One single control plane/network brain would push commands to each device, thus commanding it to manipulate its physical switching and routing hardware. In CCP a network administrator can shape traffic via a centralised console without having to touch the individual switches. The forwarding activity is decided based on the entries in flow tables, which are pre-assigned by the controller. Introduction to SDN OpenFlow Protocol: OpenFlow is a Layer 2 communications protocol that gives access to the forwarding plane of a network switch or router over the network. OpenFlow was architected for a number of devices containing only data planes to respond to commands sent to them from a (logically) centralized controller that housed the single control plane for that network. OpenFlow is a protocol specification that describes the communication between OpenFlow switches and an OpenFlow controller.

![[Pasted image 20260211212957.png]]
Introduction to SDN Interface to the Routing System (I2RS): is network programmability to create a protocol and components to act as a means of programming a network device’s routing information base (RIB). I2RS provides varying levels of abstraction in terms of programmability of network paths, policies, and port configuration, but in all cases has the advantage of allowing for adult supervision of said programming as a means of checking the commands prior to committing them. Software-defined networking (SDN): It is an architectural approach that optimizes and simplifies network operations by binding the interaction among applications, network services and network devices. SDN improves network performance and monitoring system by enabling dynamic andprogramatically efficient network configuration. It is achived by adopting logically centralised network controlwhich is called as SDN controller. Introduction to SDN Advantages: 1. Network is programmable hence can easily be modified via the controller rather than individual switches. 2. Switch hardware becomes cheaper since each switch only needs a data plane. 3. Hardware is abstracted, hence applications can be written on top of controller independent of switch vendor. 4. Provides better security since the controller can monitor traffic and deploy security policies. For example, if the controller detects suspicious activity in network traffic, it can reroute or drop the packets. Disadvantage: The central dependency of the network means single point of failure, i.e. if the controller gets corrupted, the entire network will be affected.

![[Pasted image 20260211213019.png]]
(sdn architecture image)
SDN ARCHITECTURE SDN ARCHITECTURE A typical SDN architecture consists of three layers. Application layer:It contains the typical network applications like intrusion detection, firewall, and load balancing and Business Applications Control layer:It consists of the SDN controller which acts as the brain of the network. It also allows hardware abstraction to the applications written on top of it. Infrastructure layer:This consists of physical switches which forms the data plane and carries out actual movement of data packets. The layers communicate via a set of interfaces called the northbound APIs(between application and control layer) and southbound APIs(between control and infrastructure layer). SDN ARCHIECTURE SDN architecture is Directly Programmable: Network control is directly programmable as it is separated from forwarding functions. Agile: Allows administrators to dynamically adjust network-wide traffic flow to meet the changing needs. Centrally Managed: Network intelligence is logically centralized in software based SDN controller that maintains the global view of the network, which appears as single logical switch to applications and policy engines. Programatically Configured: network managers can configure, manage, secure, and optimize network resources very quickly via dynamic, automated SDN programs, which they can write themselves because the programs do not depend on proprietary software. CONTROL PLANE Control plane establishes the local data set used to create the forwarding table entries, which are in turn used by the data plane to forward traffic between ingress (accepts network traffic) and egress(forwards the network traffic) ports on a device. The data set used to store the network topology is called the Routing Information Base (RIB). The RIB is often kept consistent (i.e., loop-free) through the exchange of information between other instances of control planes within the network. FIB:A forwarding information base (FIB), also known as a forwarding table or MAC table, is most commonly used in network bridging, routing, and similar functions to find the proper output network interface to which the input interface should forward a packet. It is a dynamic table that maps MAC addresses to ports. The FIB is a memory construct used by Ethernet switch to map a station's MAC address to the switch port the station is connected to. This allows switches to facilitate communications between connected stations at high speed. The presence of a FIB is one attribute that separates a switch from a hub. FIB are often mirrored between the control and data planes of a typical device.

![[Pasted image 20260211213059.png]]
control plane image

![[Pasted image 20260211213119.png]]
CONTROL PLANE The basic job of a network switch is to make a forwarding decision (control plane) and subsequently forward the data toward a destination (data plane). CONTROL PLANE Control and data planes of a typical network CONTROL PLANE The figure represents a network of interconnected switches. It demonstrates that packets are received by switch A on the leftmost control plane and ultimately forwarded to switch B. Packets are received on the input ports of the line card where the data plane resides. If, for example, a packet is received that comes from an unknown MAC address, it is redirected (4) to the control plane of the device, where it is learned, processed, and later forwarded onward. Once a packet has been delivered to the control plane, the information contained therein is processed and possibly results in an alteration of the RIB as well as the transmission of additional messages to its peers, alerting them of this update (i.e., a new route is learned). When the RIB becomes stable, the FIB is updated in both the control plane and the data plane. Subsequently, forwarding will be updated and reflect these. In reality, the control plane for the Internet is some combination of layer 2 or layer 3 control planes. A layer 2 control plane focuses on hardware or physical layer addresses such as IEEE MAC addresses. A layer 3 control plane is built to facilitate network layer addresses such as those of the IP protocol. CONTROL PLANE (L2 and L3) Layer 2 and Layer 3 scaling concerns and their resulting control plane designs are merge or hybridized. Layer 2 Control Plane: In a L2 network, forwarding focuses on the reachability of MAC addresses. Thus, L2 networks primarily deal with the storage of MAC addresses for forwarding purposes. Since the MAC addresses of hosts can be enormous in a large enterprise network, the management of these addresses is difficult. Hence it leads to network salability problem. Layer 3 control plane:In a L3 network, forwarding focuses on the reachability of network addresses. L3 network reachability information primarily concerns itself with the reachability of a destination IP (network) prefix. In all modern cases, L3 networking is used to segment or stitch together L2 domains in order to overcome L2 scale problems. DATA PLANE The data plane handles incoming datagrams on guided or unguided media. A correct datagram is processed in the data plane by performing lookup in the FIB table that are programmed earlier by the control plane. This is referred as referred to as the fast path for packet processing, as it just identifies the packet’s destination using the preprogrammed FIB. If packets cannot be matched to those rules, such as when an unknown destination is detected, and these packets are sent to the route processor where the control plane can further process them using the RIB. The packet forwarding in data plane is of two kinds i)Software forwarding (Switched) ii) Hardware forwarding DATA PLANE Note: Routers may have one or more processors. Higher-performance routers invariably have multiple processing elements, which may be general-purpose processor chips or specialized application-specific integrated circuits (ASIC). Very high performance products have multiple processing elements on each interface card. In such designs, the main processor does not participate in forwarding, but only in control plane and management processing. Software Forwarding: is CPU-driven forwarding of the modern dedicated network element(router/switch) which trades off a processor intensive lookup with limitless table storage of processor memory. Software switching occurs when traffic cannot be processed in hardware. Software forwarding is significantly slower than hardware forwarding, but packets forwarded by the CPU subsystem do not reduce hardware forwarding speed. Virtual Switches donot have specialised hardware to create a high-speed data path, Instead Software techniques, such as special data structures and optimised kernels, provide the forwarding path. DATA PLANE Hardware Forwarding:The data plane need to provide a high speed and low latency path. To achieve this, a lot of data plane implementation is in hardware using special ASICs (Application Specific IC). Lookups in hardware tables have proven to result in much higher packet forwarding performance and therefore have dominated network element designs, particularly for higher bandwidth network elements. The typical actions resulting from the data plane forwarding lookup are forward, drop, re-mark, count, and queue. Some of these actions may be combined or chained together. In some cases, the forward decision returns a local port, such data packets leave the hardware-forwarding path and forwarded to the route processor using an internal communications channel. DIFFERENCES B/W CP AND DP Moving Information Between Planes In systems where the control plane resides on an independent processor/line card and data planes exist on other, independent line cards, certain behaviors around the communication between these elements must exist for the system to be resilient(recover quickly) and fault tolerant. Mechanisms for detecting forwarding table distribution errors can be embedded in the data (e.g., table versioning) or in the transfer mechanism. Such mechanisms ensure that the distributed software versions of the table are synchronized and correct once programmed. Some multislot/multicard systems do two-stage lookups wherein the first stage at in gress simply identifies the outgoing slot/card on which a secondary lookup is performed. Depending on how it’s implemented, two-stage lookups can enable an optimization that allows a phenomenon called localization to reduce the egress FIB size. In these cases, scenarios around two-stage asynchronous loss may occur that require some attention and are in fact difficult to detect until they fail. These have relevance to SDN forwarding control. Distributed Control Planes In a logically distributed architecture, the controllers are physically and logically distributed. Additionally, every controller has just a view of the domain it is responsible for, and it can take decisions for it, unlike a logically centralized design, where each controller makes a decision based on the global network view. In this model, the individual elements or their proxies participate together to distribute reachability information in order to develop a localized view of a consistent, loop-free network. IP and MPLS forwarding are examples of a distributed control model. IP and MPLS: Multiprotocol Label Switching (MPLS) is a routing technique in telecommunications networks that directs data from one node to the next based on short path labels rather than long network addresses, thus avoiding complex lookups in a routing table and speeding traffic flows. MPLS can encapsulate packets of various network protocols, hence the "multiprotocol". In an MPLS network, data packets are assigned labels. Packet-forwarding decisions are made solely on the contents of this label, without the need to examine the packet itself. This allows one to create end-to-end circuits across any type of transport medium, using any protocol. MPLS operates at a layer that is generally considered to lie between traditional definitions of OSI Layer 2 and Layer 3 and thus is often referred to as a layer 2.5 protocol. Encapsulating packets with in MPLS label anything can be transported across the MPLS network regardless of the underlying protocol. DCP in IP IP control plane paradigm is to use an IGP. The IGP is used to establish reachability between a connected IP forwarding elements.Once configured, IGP protocols establish relationships with appropriately configured neighbors and manage control protocol sessions that exchange the Network Layer Reachability Information (NLRI) i.e netmask and network address for that subnet (/25, 204.149.16.128). The network elements participating in this exchange store the accumulated advertisements from other nodes in a state database (e.g., OSPF database) and run a shortest path algorithm against that data to establish a self-centered reachability graph of best paths to destinations.These best paths are contributed to the RIB. All elements speaking a particular control protocol in the domain remain connected to each other (directly or indirectly). DCP in IP Scale of the control plane state in such networks is addressed both in physical and logical design, using the tools of recursion, summarization, route filtering, and compartmentalization (physical/logical). To handle the general scale problem arising from the number of IGP neighbors supported are i)number of events that can be processed, ii) the size of the link state database or iii)other state structure and/or other related entities. To handle this elements can be divided physically/logically into areas or other IGP hierarchies. At area boundaries, the operator has the controls to summarize (if possible) reachability information from other areas information across the area borders. DCP in IP Route filtering: is a method for selectively identifying routes that are advertised or received from neighbor routers. Route filtering may be used to manipulate traffic flows, reduce memory utilization, or to improve security. Recursion:The Recursive Route Lookup follows the same logic of dividing a task into subtasks of the same type. The device performs its routing table lookup again and again until it finds the ongoing interface to reach a certain network. Recursion allows the network control plane to distribute information with different attributes specific to different protocols that link through a series of shared keys. Example: 13.0.0.0/16 via GE2/0 is non- recursive, 10.0.0.0/16 via 13.13.13.13 is recursive. DCP IN IP In the IP model, a certain amount of additional, localized control over which data in the control plane data set, is selected for forwarding in the data plane is enabled through local policies to govern the preference of learned reachability. A non-redistributed static route can affect local decisions if there is no existing route in the RIB. The combination of IGP/BGP recursion and the use of policy tools can become complex, but this is how control planes are administered in the real world. The routing configuration changes that could affect outcomes in the control and data plane can be administered centrally and pushed to the distributed elements. CONVERGENCE TIME Convergence time is a measure of how fast a group of routers reach the state of convergence. Convergence is the time it takes from when a network element introduces a change in reachability of a destination due to a network event to when this change is seen and instantiated by all other relevant network elements. The factor that affects this convergence time is propogation delay of the update which is a function of distance. Other delay components are updating of RIB and FIB. To optimize convergence processing different vendors have message update packing, update prioritization, peer update grouping, and other internal optimizations to reduce redundant update generation processing, increase the speed of convergence at the routing or control plane level. To optimize the updates to the FIB, different vendors have developed table organization strategies and event-driven reaction strategies for key components of the recursive nature of the FIB to minimize the number and type of changes to the FIB LOAD BALNCING & AVAILABILITY IN IP Load balancing is normally applied to equal cost paths or bundled point-to-point circuits. The actual efficiency of a load balancing algorithm is bounded by both the computation algorithm itself, as well as the potential imbalances in flow size an implementation might encounter. High Availability: availability is used to describe the period of time when a network service is available. High availability is provided through Redundancy at the network level:redundant routers/switches and redundant paths in the network design allow for the failure of a link or element Redundancy at the element level: using redundant route processors/switch control modules. The redundant processors can work in either a stateless active/standby mode. Multiprotocol Label Switching (MPLS) In the traditional packet-forwarding paradigm, an independent forwarding decision is made at each hop as a packet travels from one switch to the next,. The IP network header is analyzed and the next hop is chosen based on this analysis and on the information in the routing table. In an MPLS environment, the analysis of the packet header is made only once, when a packet enters the MPLS tunnel (that is, the path used for MPLS traffic). When an IP packet enters a

![[Pasted image 20260211213212.png]]
MLPS label switching diagram
label-switched path (LSP), the ingress provider edge (PE) switch examines the packet and assigns it a label based on its destination, placing the label in the packet’s header.Intermediate MPLS N/W device then replace the old label with a new label and forward the packet to the next switch in the path. When the packet reaches the egress PE switch, the label is removed, and the packet again becomes a native IP packet and is forwarded based on its IP routing information. Multiprotocol Label Switching (MPLS) Label-switched paths (LSPs) are established by the network operator to create network-based IP virtual private networks or to route traffic along specified paths through the network. In many respects, LSPs are not different from permanent virtual circuits (PVCs) MPLS Frame Format
![[Pasted image 20260211213231.png]]
frame format diagram
The label is 24 bits, which means there are 1,048,575 labels, Labels can be stacked in a LIFO (last in, first out) order. The stacking of labels allows for the creation of multiple services or tunnels across a network. MPLS Operations 1. Push operation affixes a new label to the top of the IP packet. For IPv4 packets, the new label is the first label. The time to live (TTL) field value in the packet header is derived from the IP packet header. 2. Pop operation removes a label from the beginning of the packet. Once the label is removed, the TTL is copied from the label into the IP packet header, and the underlying IP packet is forwarded as a native IP packet 3. Swap operation removes an existing MPLS label from an IP packet and replaces it with a new MPLS label, based on the following: Incoming interface Label Label forwarding table MLPS Operations

![[Pasted image 20260211213257.png]]
mlps ops diagram
IP packet without a label arriving on the customer edge interface (ge-0/0/1) of the ingress PE switch. The ingress PE switch examines the packet and identifies that packet’s destination as the egress PE switch. The ingress PE switch applies label 100 to the packet and sends the MPLS packet to its outgoing MPLS core interface (ge-0/0/5). The MPLS packet is transmitted on the MPLS tunnel through the provider switch, where it arrives at interface ge-0/0/5 with label 100. The provider switch swaps label 100 with label 200 and forwards the MPLS packet through its core interface (ge-0/0/7) to the next hop on the tunnel, which is the egress PE switch. The egress PE switch receives the MPLS packet through its core interface (ge-0/0/7), removes the MPLS label, and sends the IP packet out of its customer edge interface (ge0/0/1) to a destination that is beyond the tunnel. DCP in MPLS The main aspects of MPLS operation involve label allocation, address binding, and label distribution and these aspects can be controlled by configuration. Label allocation is normally dynamic, but label scale can be controlled somewhat in some vendor implementations particularly in the context of VPNs The label distribution protocols can be LDP, RSVP (and BGP for the labeled unicast address family). These control protocols have neighbor/session forming behaviors and information exchange Label Distribution Protocol (LDP) is a protocol in which routers capable of Multiprotocol Label Switching (MPLS) exchange label mapping information. Two routers with an established session are called LDP peers and the exchange of information is bi-directional. LDP is used to build and maintain LSP databases that are used to forward traffic through MPLS networks. Resource Reservation Protocol (RSVP) is a signaling protocol that reserves resources, such as for IP unicast and multicast flows, and requests quality-of-service (QoS) parameters for applications. Senders send a PATH message from the source to the receiver to specify the reservation requirements of a data flow. Receivers send a RESV message to the sender to reserve resources for the flow. Label distribution can be downstream on-demand or downstream unsolicited which is the default behavior of LDP. DCP in MLPS Aspects of MPLS control plane behavior can be controlled by global and local configuration by filtering label advertisements, control label retention policy, control label range and use & distribution of reserved labels. The network element can perform label actions that include push, pop, swap, multiple push, and swap-and-push (in addition to forward). When MPLS is deployed, the forwarding behavior of the data plane changes from longest destination prefix match to a match of the topmost label on the label stack. MLPS adds complexity by maintaining additional tables and references between the IP forwarding table and the label table. MPLS also adds to the overall complexity of the distributed IP control paradigm. The specific application of MPLS traffic-engineered tunnels allows the operator to control the path of tunnels. These MPLS tunnels are loaded based on the next hop address of a class of prefixes, called a Forwarding Equivalence Class (FEC). A FEC can also be a set of policies that specifically identify specific flows or quality of service characteristics of the flows such as those used by policy-based routing.
